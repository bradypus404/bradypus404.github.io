---
layout: post
title: "[ML Study]ch03-2 선형 회귀"
subtitle: 혼자 공부하는 머신러닝+딥러닝 [3-2]
date: 2023-07-13 09:01:00 +0900
author: Hyemin
categories: [Machine Learning]
tags: [선형 회귀, 계수 또는 가중치, 모델 파라미터, 다항 회귀]
---
본 포스팅 내용은 [`혼자공부하는 머신러닝 딥러닝(박해선 저)`](https://product.kyobobook.co.kr\detail/S000001810330)를 함께 스터디하고 정리한 내용입니다. 

## 선형 회귀
------------------
### k-최근접 이웃의 한계

- k-최근접 이웃 알고리즘은 어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용한다.
- 장점으로는 단순하고 효율적이며, 새로운 데이터가 들어왔을 때 주변 데이터들을 보고 판단하기 때문에 훈련이 필요 없다.
- 단점으로는 k의 값에 따라 판단하기 때문에 k의 값을 선택하는 것이 중요하고, 데이터 간의 범위가 크면 판단하는데 어려움이 생길 수 있다.


- 아래의 농어 데이터를 이용해 k-최근접 이웃 알고리즘의 한계를 알아보고자 한다.
```python
import numpy as np

perch_length = np.array(
    [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0,
     21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5,
     22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5,
     27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0,
     36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0,
     40.0, 42.0, 43.0, 43.0, 43.5, 44.0]
     )
perch_weight = np.array(
    [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0,
     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0,
     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0,
     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0,
     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0,
     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0,
     1000.0, 1000.0]
     )
```

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state=42)
train_input = train_input.reshape(-1, 1)
test_input = test_input.reshape(-1, 1)
```
- 데이터를 훈련 세트와 테스트 세트로 나누고 특성 데이터는 2차원 배열로 변환한다.

```python
from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor(n_neighbors=3)
knr.fit(train_input, train_target)
```
- 최근접 이웃 개수를 3으로 하는 모델을 훈련한다.
- 이 모델을 사용해 길이가 50cm인 농어의 무게를 예측하고자 한다.

![예측값](/assets/images/post/2023-07-19[3-2]/1.예측값.png)

- 이 모델은 50cm인 농어의 무게를 1,033g 정도로 예측했다. 
- 하지만 실제 이 농어의 무게는 훨씬 더 많이 나간다.
- 왜 이런 문제가 발생했는지 계속해서 알아본다.

```python
import matplotlib.pyplot as plt

distances, indexes = knr.kneighbors([[50]])

plt.scatter(train_input, train_target)
plt.scatter(train_input[indexes], train_target[indexes], marker='D')

plt.scatter(50, 1033, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
- 훈련 세트와 50cm인 데이터 그리고 이 농어의 최근접 이웃을 산점도에 표시한다.
- kneighbors() 메서드를 사용하면 가장 가까운 이웃까지의 거리와 이웃 샘플의 인덱스를 얻을 수 있다.

![예측 산점도1](/assets/images/post/2023-07-19[3-2]/2.예측%20산점도1.png)


- 길이가 50cm이고 무게가 1,033g인 농어는 marker='^' 로 표시되고, 그 주변의 샘플은 marker='D' 이다.
- 산점도를 보면 길이가 커질수록 농어의 무게가 증가하는 경향이 있다는 걸 알 수 있다.
- 어쩌면 당연한 결과지만 50cm 농어에서 가장 가까운 것은 45cm근방이기 때문에 k-최근접 이웃 알고리즘은 이 샘플들의 무게를 평균한다. 
- 그러면 이웃 샘플의 타깃의 평균을 구해본다.

![이웃샘플평균](/assets/images/post/2023-07-19[3-2]/3.이웃샘플평균.png)

- 평균을 구해보면 모델이 예측한 값과 같은 것을 볼 수 있다.
- k-최근접 이웃 회귀는 가장 가까운 샘플을 찾아 타깃을 평균한다.
- 따라서 새로운 샘플이 훈련 세트의 범위를 벗어나면 엉뚱한 값을 예측할 수 있다.
- 예를 들어 길이가 100cm인 농어도 1,033g으로 예측한다.

![100샘플평균](/assets/images/post/2023-07-19[3-2]/4.길이100cm예측.png)

- 길이를 100cm의 예측값을 보면 1,033g으로 예측하는 것을 볼 수 있다.
- 한 번 더 산점도를 그려 확인한다.

```python
distances, indexes = knr.kneighbors([[100]])

plt.scatter(train_input, train_target)
plt.scatter(train_input[indexes], train_target[indexes], marker='D')

plt.scatter(100, 1033, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![100샘플평균](/assets/images/post/2023-07-19[3-2]/5.예측%20산점도2.png)

- 이렇게 하면 농어의 길이가 아무리 커도 무게가 더 늘어나지 않는다.
- k-최근접 이웃을 사용해 이 문제를 해결하려면 가장 큰 농어가 포함되도록 훈련 세트를 다시 만들어야 한다.
- 이러한 방법은 새로운 데이터가 들어올 때마다 훈련 세트를 매번 만들어야 하기 때문에 비효율적이다.
- 이 문제를 해결할 다른 알고리즘을 찾아보고자 한다.

-------------------
### 선형 회귀

**선형 회귀**(linear regression)은 널리 사용되는 대표적인 회귀 알고리즘이다.

- 특성이 하나인 경우 어떤 직선을 학습하는 알고리즘이다. 
    - 어떤 직선은 특성을 가장 잘 나타낼 수 있는 직선을 찾아야 한다.

![선형회귀산점도](/assets/images/post/2023-07-19[3-2]/6,선형회귀그래프.png)
 
- 직선의 위치가 만약 훈련 세트의 평균에 가깝다면 R²은 0에 가까운 값이 된다.
- 위 산점도처럼 최고의 직선을 머신러닝 알고리즘이 자동으로 찾을 수 있다.

```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(train_input, train_target)

print(lr.predict([[50]]))
```
![선형회귀예측](/assets/images/post/2023-07-19[3-2]/7.선형회귀훈련.png)

- 사이킷런은 sklearn.linear_model 패키지 아래에 LinearRegression 클래스로 선형 회귀 알고리즘을 구현 했다.
    - LinearRegression 클래스에도 fit(), score(), predict() 메서드가 존재한다.
- k-최근접 이웃 회귀를 사용했을 때와 달리 선형 회귀는 50cm 농어의 무게를 아주 높게 예측하였다.

- 하나의 직선을 그리려면 기울기와 절편이 있어야 한다.
- **y = a * x + b** 처럼 할 수 있다. x를 농어의 길이, y를 농어의 무게로 바꾸면 아래의 그림과 같다.

![직선의 방정식](/assets/images/post/2023-07-19[3-2]/8.%20직선의%20방정식.png)

![a와b](/assets/images/post/2023-07-19[3-2]/9.LinearRegression찾은a와b.png)

- LinearRegression 클래스가 찾은 a와 b는 lr 객체의 coef_와 intercept_ 속성에 저장되어 있다.
- 머신러닝에서 기울기를 **계수(coefficient)** 또는 **가중치(weight)**라고 부른다.

conf_와 intercept_를 머신러닝 알고리즘이 찾은 값이라는 의미로 **모델 파라미터(model parameeter)**라고 부른다. 

**모델 기반 학습** : 많은 머신러닝 알고리즘의 훈련 과정과 최적의 모델 파리미터를 찾는 것을 말한다.
**사례 기반 학습**: k-최근접 이웃은 모델 파라미터가 없다. 훈련 세트를 저장하는 것이 훈련의 전부이다.
이처럼 최적의 모델 파라미터를 찾는 과정 없이 훈련 샘플을 이용하여 새로운 데이터와 비교하는 학습을 말한다. 

```python
plt.scatter(train_input, train_target)
plt.plot([15, 50], [15*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])

plt.scatter(50, 1241.8, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

- 앞에서 구한 기울기와 절편을 이용하여 길이 15에서 50까지의 직선을 그려 산점도를 그린다.
- plt.plot()에서 (15, 15 * 39 - 709) 와 (50, 50 * 39 - 709) 두 점을 이으면 된다.

![기울기와절편산점도](/assets/images/post/2023-07-19[3-2]/10.기울기와절편을사용한산점도.png)

- 위 그래프의 직선이 선형 회귀 알고리즘이 이 데이터셋에서 찾은 최적의 직선이다.
- 위 과정을 통해 훈련 세트 범위를 벗어난 농어의 무게를 예측할 수 있다.

![lr스코어](/assets/images/post/2023-07-19[3-2]/11.lr스코어.png)

- 훈련 세트와 데스트 세트에 대한 R² 점수를 확인해보면 차이가 나는 것을 확인할 수있다.
- 이 모델이 훈련 세트에 과대적합라고 생각할 수 있지만, 훈련 세트의 점수도 높지 않아 과소적합되었다고 말할 수 있다.
- 하지만 과소적합 말고 위 그래프의 왼쪽 아래를 보면 또 다른 문제가 있음을 알 수 있다.

------------------
### 다항 회귀

- 위 그래프에서 선형 회귀가 만든 직선이 왼쪽 아래로 쭉 뻗어 있다. 이 직선대로 예측하면 농어의 무게가 0g 이하로 내려갈 텐데 0g 이하는 있을 수 없다.
- 그래프를 보면 데이터들이 일직선보다 곡선의 형태를 띄고 있다. 그러면 최적의 직선보다 최적의 곡선을 찾는게 더 정확한 값을 구할 수 있을 것이다. 

![다항회귀](/assets/images/post/2023-07-19[3-2]/12.다항회귀.png)

- 위 그림과 같은 2차 방정식의 그래프를 그리려면 길이를 제곱한 항이 훈련 세트에 추가되어야 한다.
- numpy를 사용하면 간단히 만들 수 있다.

```python
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))
```

- 위 코드 **train_input ** 2** 식에서 넘파이 브로드캐스팅이 적용된다. 즉 train_input에 있는 모든 원소를 제곱한다.  

![데이터셋 크기](/assets/images/post/2023-07-19[3-2]/13.데이터셋%20크기.png)

- 원래 특성인 길이를 제곱하여 왼쪽 열에 추가했기 때문에 훈련 세트와 테스트 세트 모두 열이 2개로 늘어난 것을 확인할 수 있다. 
- 이제 train_poly를 사용해 선형 회귀 모델을 다시 훈련한다.
- 여기서 중요한 것은 2차 방정식 그래프를 찾기 위해 훈련 세트에 제곱 항을 추가했지만, 타깃값은 그대로 사용한다는 것이다.
- 선형 회귀 모델을 훈련한 다음 50cm 농어에 대해 무게를 다시 예측해본다.

![다항회귀lr값](/assets/images/post/2023-07-19[3-2]/14.lr값구하기.png)

- 예측한 값을 보면 전 모델보다 더 높은 값을 예측한 것을 확인할 수 있다. 

![다항회귀 계수와 절편](/assets/images/post/2023-07-19[3-2]/15.다항회귀%20훈련한%20계수와%20절편.png)

- 이 모델이 훈련한 계수와 절편을 출력하면 위와 같은 값을 확인할 수 있다.
- 이 모델은 다음과 같은 그래프를 학습했다.

**무게 = 1.01 * 길이² - 21.6 * 길이 + 116.05** 

- 위 식은 2차 방정식인데 비선형(non-linear)라고 의문이 들 수 있다. 
- 길이²을 간단히 다른 변수로 치환할 수 있다.
- '길이² = 왕길이'로 바꾸면 [무게 = 1.01 * 왕길이 - 21.6 * 길이 + 116.05]와 같이 작성할 수 있다. 즉 무게는 왕길이와 길이의 선형 관계로 표현할 수 있다.

이런 방정식을 **다항식(polynomial)**이라 부르며 다항식을 사용한 선형 회귀를 **다항 회귀(polynomial regression)**라고 부른다.

```python
point = np.arange(15, 50)

plt.scatter(train_input, train_target)
plt.plot(point, 1.01*point**2 - 21.6*point + 116.05)

plt.scatter([50], [1574], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![다항회귀 산점도](/assets/images/post/2023-07-19[3-2]/16.다항회귀산점도.png)

- 위 식을 구한 후 다시 산점도를 그려보면 단순 선형 회귀 모델보다 나은 그래프가 그려진 것을 확인할 수 있다.
- 훈련 세트의 경량을 잘 따르고 무게가 음수로 나오는 일도 없을 것이다.

![다항회귀 lr 스코어](/assets/images/post/2023-07-19[3-2]/17.lr스코어.png)

- 훈련 세트와 테스트 세트의 R² 점수를 확인해보면 전보다 크게 높아진 것을 볼 수 있다. 
- 하지만 여전히 테스트 세트의 점수가 높은 것을 보면 과소적합이 아직 남아 있다는 것을 알 수 있다.
- 이 문제를 해결하기 위해서는 더 복잡한 모델이 필요하다.



