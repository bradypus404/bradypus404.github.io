---
layout: post
title: "[ML Study]ch04-1 로지스틱 회귀"
subtitle: 
date: 2023-07-20 09:01:00 +0900
author: dschoi
categories: [Machine Learning]
tags: [로지스틱 회귀, 다중분류, 시그모이드 함수, 소프트맥스 함수]
---



```python
# 데이터 준비
import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Species</th>
      <th>Weight</th>
      <th>Length</th>
      <th>Diagonal</th>
      <th>Height</th>
      <th>Width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Bream</td>
      <td>242.0</td>
      <td>25.4</td>
      <td>30.0</td>
      <td>11.5200</td>
      <td>4.0200</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Bream</td>
      <td>290.0</td>
      <td>26.3</td>
      <td>31.2</td>
      <td>12.4800</td>
      <td>4.3056</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Bream</td>
      <td>340.0</td>
      <td>26.5</td>
      <td>31.1</td>
      <td>12.3778</td>
      <td>4.6961</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Bream</td>
      <td>363.0</td>
      <td>29.0</td>
      <td>33.5</td>
      <td>12.7300</td>
      <td>4.4555</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Bream</td>
      <td>430.0</td>
      <td>29.0</td>
      <td>34.0</td>
      <td>12.4440</td>
      <td>5.1340</td>
    </tr>
  </tbody>
</table>
</div>




```python
print(pd.unique(fish['Species']))
```

    ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']



```python
fish_target = fish['Species'].to_numpy()
fish_target
```




    array(['Bream', 'Bream', 'Bream', 'Bream', 'Bream', 'Bream', 'Bream',
           'Bream', 'Bream', 'Bream', 'Bream', 'Bream', 'Bream', 'Bream',
           'Bream', 'Bream', 'Bream', 'Bream', 'Bream', 'Bream', 'Bream',
           'Bream', 'Bream', 'Bream', 'Bream', 'Bream', 'Bream', 'Bream',
           'Bream', 'Bream', 'Bream', 'Bream', 'Bream', 'Bream', 'Bream',
           'Roach', 'Roach', 'Roach', 'Roach', 'Roach', 'Roach', 'Roach',
           'Roach', 'Roach', 'Roach', 'Roach', 'Roach', 'Roach', 'Roach',
           'Roach', 'Roach', 'Roach', 'Roach', 'Roach', 'Roach', 'Whitefish',
           'Whitefish', 'Whitefish', 'Whitefish', 'Whitefish', 'Whitefish',
           'Parkki', 'Parkki', 'Parkki', 'Parkki', 'Parkki', 'Parkki',
           'Parkki', 'Parkki', 'Parkki', 'Parkki', 'Parkki', 'Perch', 'Perch',
           'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch',
           'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch',
           'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch',
           'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch',
           'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch',
           'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch',
           'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Perch',
           'Perch', 'Perch', 'Perch', 'Perch', 'Perch', 'Pike', 'Pike',
           'Pike', 'Pike', 'Pike', 'Pike', 'Pike', 'Pike', 'Pike', 'Pike',
           'Pike', 'Pike', 'Pike', 'Pike', 'Pike', 'Pike', 'Pike', 'Smelt',
           'Smelt', 'Smelt', 'Smelt', 'Smelt', 'Smelt', 'Smelt', 'Smelt',
           'Smelt', 'Smelt', 'Smelt', 'Smelt', 'Smelt', 'Smelt'], dtype=object)




```python
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
```


```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
```


```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)

print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))
```

    0.8907563025210085
    0.85


---

## 로지스틱 회귀
---

- `로지스틱 회귀(Logistic Regression)`은 회귀라는 용어를 사용하지만 실제로는`분류 모델`을 의미한다. 
  - target 데이터가 2개 이상의 클래스가 포함된 경우 `다중 분류(Multi-Class Classification)`이라 한다.
- 선형 회귀와 동일하게 선형 방정식을 모델이 학습하는 알고리즘이다. 
  - ex) 물로기 데이터 예시 선형 방정식: $z = a * (Weight) + b * (Length) + c * (Diagonal) + d * (Height) + e * (Width) + f$
  - a, b, c, d, e, f는 모두 `가중치`를 의미한다. 
  - 확률을 표시하기 위해 `Sigmoid` 함수를 이용하여 z 값이 0 ~ 1 사이 값이 되어야 한다. 



```python
import numpy as np
import matplotlib.pyplot as plt

z = np.arange(-5, 5, 0.1)
phi = 1 / (1 + np.exp(-z))

plt.plot(z, phi)
plt.xlabel('z')
plt.ylabel('phi')
plt.title('Sigmoid')
plt.show()
```


    
![png](/assets/images/post/2023-07-20-ch04-1/output_11_0.png)
    


### 로지스틱 회귀로 `이진 분류` 수행하기
---

- numpy 배열은 True, False 값을 통해 행을 선택할 수 있으며 이를 불리언 인덱싱(Boolean Indexing)이라 한다. 
- 다음 불리언 인덱싱 예시 코드이다. True, False 값을 통해 문자 'A', 'C'를 선택할 수 있다.


```python
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])
```

    ['A' 'C']



```python
# 도미(Bream), 빙어(Smelt) 행을 골라내는 코드
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```

- 위의 코드를 통해 bream, smelt만 포함된 train, target 데이터를 생성할 수 있다.  
- 준비된 데이터를 다음 코드를 통해 로지스특 회귀 모델에 학습한다.


```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```




<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression()</pre></div></div></div></div></div>




```python
print(lr.predict(train_bream_smelt[:5]))
```

    ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']


- 2번째 데이터를 제외하고 모두 도미로 예측하였다.
- `KNeighborsClassifier`와 마찬가지로 예측 확률은 predict_proba() 메소드에서 제공한다. 
- 다음 코드는 처음 5개의 샘플에 대한 확률 값을 보여준다.


```python
print(lr.predict_proba(train_bream_smelt[:5]))
```

    [[0.99759855 0.00240145]
     [0.02735183 0.97264817]
     [0.99486072 0.00513928]
     [0.98584202 0.01415798]
     [0.99767269 0.00232731]]


- 첫 번째 열은 음성 클래스 (0)을 의미하고, 두 번째 열은 양성 클래스 (1)을 의미한다.
- `사이킷런은 target 값을 알파벳순으로 정렬`한다. 따라서 위의 코드에서 빙어(Smelt)가 양성 (1)를 의미한다.  


```python
print(lr.classes_)
```

    ['Bream' 'Smelt']



```python
print(lr.coef_, lr.intercept_)
```

    [[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]


- lrcoef_와 lr.intercept_는 로지스특 회귀 함수가 학습한 계수(또는 가중치)를 보여준다.
- 주어진 계수를 이용하여 z 값을 계산할 수 있는데 이때 `LogisticRegeression` 클래스의 `decision_function()` 메소드를 이용할 수 있다.


```python
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)
```

    [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]



```python
from scipy.special import expit

print(expit(decisions))
```

    [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]


- decision_function을 통해 구해진 z값을 `scipy 라이브러리`의 시그모이드 함수에 전달하면 확률 값을 계산할 수 있다.
- decision_function()은 양성 클래스에 대한 z 값을 반환한다. 
- 앞선 과정을 통해 이진 분류를 위한 샘플 2개를 선정하였고, 이를 통해 로지스틱 회귀 모델을 훈련하였다. 
- 이진 분류에서는 predict_proba() 메소드를 통해 양성 클래스와 음성 클래스에 대한 확률을 얻을 수 있다.
- 또한, decision_function() 메소드는 양성 클래스에 대한 z값을 계산한다. 

### 로지스틱 회귀로 다중 분류 수행하기
---
- LogisticRegression 클래스는 기본적으로 반복적인 알고리즘을 사용한다.
- `max_inter` 매개변수에서 반복 횟수를 지정하며 기본값은 100이다. 
- 또한, 기본적으로 릿지 회귀와 같이 계수의 제곱을 규제한다. 규제를 제어하는 매개변수는 C이다. 기본값은 1을 갖는다.


```python
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

    0.9327731092436975
    0.925


- 훈련 세트와 테스트 세트에 대한 점수가 높고, 과대적합 또는 과소적합에 치우치지 않은 결과를 확인할 수 있다.
- 다음 코드는 테스트 세트의 처음 5개 데이터 샘플을 보여주며, 예측 확률을 보여준다.


```python
print(lr.predict(test_scaled[:5]))
```

    ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']



```python
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))
```

    [[0.    0.014 0.841 0.    0.136 0.007 0.003]
     [0.    0.003 0.044 0.    0.007 0.946 0.   ]
     [0.    0.    0.034 0.935 0.015 0.016 0.   ]
     [0.011 0.034 0.306 0.007 0.567 0.    0.076]
     [0.    0.    0.904 0.002 0.089 0.002 0.001]]


- 앞서 설명한 바와 같이 각 열은 class의 확률을 나타내며, 확률 값이 높은 것이 해당 클래스에 속함을 의미한다. 
- 다음 코드를 통해 각 열의 클래스 값을 확인할 수 있다.


```python
print(lr.classes_)
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']


- 앞선 코드를 다시 살펴보면 두 번째 샘플의 여섯 번째 열은 0.946의 확률로 Smelt에 속할 확률이 가장 높은 것을 볼 수 있다.
- 이처럼 다중 클래스 분류에서는 확률 값을 통해 이진 분류와 마찬가지 방식으로 1에 가까운 값을 갖는 클래스로 데이터를 분류하는 것을 확인할 수 있다.
- 다중 분류에서는 각 클래스마다 z 값을 하나씩 계산하게 된다. 
- 다음 코드를 통해 해당 내용을 확인할 수 있다.


```python
print(lr.coef_.shape, lr.intercept_.shape)
```

    (7, 5) (7,)


- 행을 나타내는 값이 7이기 때문에 이는 z값이 7개 있다는 것을 의미한다. 
- 즉, 각 클래스별로 z값을 각각 계산한다는 것을 의미한다. 
- 여기서 중요한 점은 이진 분류에서는 주로 시그모이드 함수를 이용하여 확률을 계산하지만, 다중 분류 문제에서는 `소프트맥스(Softmax)` 함수를 사용하여 여러 클래스의 z값을 확률로 변환한다는 점이다. 
  - 시그모이드 함수는 하나의 선형 방정식의 출력값을 0 ~ 1 사이값으로 압축한다. 
  - 반면 소프트맥스 함수는 여러 개의 선형 방정식의 출력값을 0 ~ 1사이로 압축한 뒤, 전체 합이 1이 되도록 만든다. 이를 위해 지수 함수를 사용하기 때문에 정규화된 지수 함수라고도 한다.

- 다음 코드는 이진 분류에서 사용했던 decision_function()을 이용하여 z1 ~ z7까지의 값을 구한 뒤, 소프트맥스 함수를 이용하여 확률을 계산한 결과를 보여준다.


```python
decision = lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals=2))
```

    [[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
     [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
     [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
     [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
     [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]



```python
from scipy.special import softmax

proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
```

    [[0.    0.014 0.841 0.    0.136 0.007 0.003]
     [0.    0.003 0.044 0.    0.007 0.946 0.   ]
     [0.    0.    0.034 0.935 0.015 0.016 0.   ]
     [0.011 0.034 0.306 0.007 0.567 0.    0.076]
     [0.    0.    0.904 0.002 0.089 0.002 0.001]]


- `proba` 출력 결과를 살펴보면 앞서 설명한 proba = lr.predict_proba(test_scaled[:5]) 코드의 결과와 동일한 확률 값을 보이는 것을 확인할 수 있다.
