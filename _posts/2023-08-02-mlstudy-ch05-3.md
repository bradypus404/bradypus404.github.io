---
layout: post
title: "[ML Study]ch05-3 트리의 앙상블"
subtitle: 혼자 공부하는 머신러닝+딥러닝 [5-3]
date: 2023-08-01 09:03:00 +0900
author: Hyeonsu
categories: [Machine Learning]
tags: [앙상블 학습, 랜덤 포레스트, 엑스트라 트리, 그레이디언트 부스팅]
---
<body>
    <img
    src="/assets/images/post/book_banner.jpg"
    align="right"
    width="20%"
    height="27.2%"
    />
    <br><br>
    <p>본 포스팅 한빛미디어의 <a href="https://product.kyobobook.co.kr/detail/S000001810330"><혼자공부하는 머신러닝+딥러닝(박해선 저)></a>를 요약 정리했습니다.</p>
</body>
<br>

### 1. 정형 데이터와 비정형 데이터 
---------------------------

**정형 데이터**는 어떤 구조로 되어있다는 뜻이며, CSV나 데이터베이스, 엑셀 등에 저장하기 쉽다.
<br>**비정형 데이터**는 텍스트 데이터, 디지털 카메라로 찍은 사진 등 데이터베이스나 엑셀로 표현하기 어려운 것들이다.
<br>
정형 데이터를 다루는데 가장 뛰어난 성과를 내는 알고리즘은 **앙상블 학습(ensemble learning)**이다.

### 2. 랜덤 포레스트
---------------------------

**랜덤 포레스트(Random Forest)**는 대표적인 앙상블 학습이다.
<br>결정 트리를 랜덤하게 만들어 결정 트리(나무)의 숲을 만든다.
<br>각 결정 트리의 예측을 사용해 최종 예측을 만든다.

각 트리를 훈련하기 위한 데이터를 랜덤하게 만드는데,
<br>우리가 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만든다.
<br>한 샘플이 중복되어 추출이 가능하다.

**부트스트랩 샘플(bootstrap sample)**은 중복된 샘플을 뽑을 수 있는 것으로,
<br>부트스트랩 샘플을 훈련 세트의 크기와 같게 만든다.
<br>1000개의 샘플 가방에서 중복하여 1000개의 샘플을 뽑는다.

분류 모델인 RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근만큼의 특성을 선택한다.
<br>회귀 모델인 RandomForestRegressor는 전체 특성을 사용한다.

랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 막아준다.
<br>결정 트리의 앙상블이기 때문에 DecisionTreeClassifier가 제공하는 중요한 매개변수를 모두 제공한다.
<br>또한 **특성 중요도**는 각 결정 트리의 특성 중요도를 취합한 것이다.
<br>특성의 일부를 랜덤하게 선택하여 결정 트리를 훈련하기 때문에 하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회를 얻는다.

부트스트랩 샘플에 포함되지 않고 남는 샘플을 OOB 샘플이라고 하는데,
<br>이것을 이용해 부트스트랩 샘플로 훈련한 결정 트리를 평가할 수 있다.


### 3. 엑스트라 트리
---------------------------

**엑스트라 트리(Extra Trees)**는 랜덤 포레스트와 매우 비슷하게 동작한다.
<br>랜덤 포레스트와의 차이점은 부트스트랩 샘플을 사용하지 않는다는 점이 있다.
<br> 각 결정 트리를 만들 때 전체 훈련 세트를 사용하고,대신 노드를 분할할 때 무작위로 분할한다.
<br> 성능이 낮아지겠지만 많은 트리를 앙상블하기 때문에 검증 세트의 점수를 높이는 효과를 가진다.

ExtraTreesClassifier로 교차 검증 점수를 확인하면 랜덤 포레스트와 비슷한 결과를 보여준다.
<br>랜덤하게 노드를 분할하기 때문에 빠른 계산 속도가 장점이다.
<br>엑스트라 트리의 회귀 버전은 ExtraTreesRegressor 클래스이다.


### 4. 그레이디언트 부스팅
---------------------------

**그레이디언트 부스팅(gradient boosting)**은 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이다.
<br>GradientBoostingClassifier는 기본적으로 깊이가 3인 결정 트리를 100개 사용한다.
<br>경사 하강법을 사용해 트리를 앙상블에 추가한다.
<br>분류에서는 로지스틱 손실 함수를 사용하고 회귀에서는 평균 제곱 오차 함수를 사용한다.

그레이디언트 부스팅은 결정 트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동한다.
<br>깊이가 얕은 트리를 사용하는 것도 그 이유이다.
<br>또한 학습률 매개변수로 속도를 조절한다.

GradientBoostingClassifier 사용해 교차 검증 점수를 확인하면 거의 과대적합이 되지 않는다.
<br>결정 트리 개수를 늘려도 과대적합을 잘 억제하고 랜덤 포레스트보다 일반적으로 조금 더 높은 성능을 보인다. 
<br>하지만 훈련 속도가 느리다는 단점이 있다.

### 5. 히스토그램 기반 그레이디언트 부스팅
---------------------------

**히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)**은 입력 특서을 256개 구간으로 나누어 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다,
<br>입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없게 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용한다.

과대적합을 잘 억제하면서 그레이디언트 부스트보다 조금 더 높은 성능을 제공한다.
<br>HistGradientBoostingClassifier 테스트 세트에서의 성능을 확인해보면 약 87% 정확도를 얻었다.
<br>앙상블 모델은 단일 결정 트리보다 좋은 결과를 얻을 수 있다.

사이킷런 말고 그레이언트 부스팅 알고리즘을 구현할 라이브러리가 있는데,
<br>XGBoost라고 코랩에서 사용할 수 있다.
<br>또 다른 히스토그램 기반 그레이디언트 부스팅 라이브러리는 LightGBM이 있다.