---
layout: post
title: "[ML Study]ch02-2 데이터 전처리"
subtitle: 
date: 2023-07-13 09:00:00 +0900
author: dschoi
categories: [Machine Learning]
tags: [데이터 전처리, 표준점수, 브로드캐스팅]
---


## 데이터 전처리
- 올바른 결과 도출을 위해 데이터 사용 전 데이터 전처리 과정이 필요하다.
- 특성의 스케일 변환 방법을 적용하는 것이 중요하다.

```python
# sample data
fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0,
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0,
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8,
                10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0,
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7,
                7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
```

### `colmumn_stack()` 함수
---
- 전달받은 리스트를 일렬로 세운 다음 차례대로 나란히 연결하는 함수
- 연결할 리스트는 Tuple로 전달


```python
# 예시 코드
import numpy as np
np.column_stack(([1,2,3], [4,5,6]))
```




    array([[1, 4],
           [2, 5],
           [3, 6]])



9`column_stack()` 함수를 이용하여 두 개의 리스트를 Tuple로 전달한 결과
- [1,2,3]과 [4,5,6] 리스트를 일렬로 세운 후, 옆으로 연결한 형태가 된다.
- 3개의 행과 2개의 열을 갇는 np.array가 된다.

fish_data를 np.column_stack()으로 연결하면 다음과 같은 결과를 얻을 수 있다.


```python
fish_data = np.column_stack((fish_length, fish_weight))
print(fish_data[:5])
```

    [[ 25.4 242. ]
     [ 26.3 290. ]
     [ 26.5 340. ]
     [ 29.  363. ]
     [ 29.  430. ]]


각 행은 fish의 샘플을 의미하게 되고, 열은 fish_length와 fish_weight을 의미하게 된다.

### `np.ones(), np.zeros()` 함수
---
- np.ones(): 배열의 모든 값을 1로 채움
- np.zeros(): 배열의 모든 값을 0으로 채움

### `np.concatenate() 함수`
---
- 두 개의 배열을 연결해주는 기능을 제공


```python
fish_target = np.concatenate((np.ones(35), np.zeros(14)))
print(fish_target)
```

    [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
     1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
     0.]


### scikit-learn을 이용한 Training Dataset과 Test Dataset 나누기
---
- [`sckit-learn`](https://scikit-learn.org/stable/#): 머신러닝을 위한 다양한 알고리즘 및 함수를 제공ㅎ
- `train_test_spilit()`
    - 데이터세트를 훈련 세트와 테스트 세트로 나눠주는 기능을 제공하는 함수, 데이터 세트를 적절하게 섞어서 나누어 주는 기능 제공
    - Default로 `25%를 테스트 세트`로 분할
    - model_selection 모듈 안에 있음
    - `from sklearn.model_selection import train_test_split`으로 모듈 임포트
    - 사용방법: 나누고 싶은 리스트나 배열 형태의 데이터 세트드를 원하는 만큼 전달
    - 다음코드는 fish_data와 fish_target 2개의 배열을 전달하여 train_input, test_input, train_target, test_target 4개의 배열 반환


```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state=42)

print(train_input.shape, test_input.shape)
```

    (36, 2) (13, 2)



```python
print(train_target.shape, test_target.shape)
```

    (36,) (13,)


- 훈련 데이터와 테스트 데이터를 각각 36개와 13개 데이터로 분할한다.


```python
print(test_target)
```

    [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]


- target 데이터가 적절히 섞여서 출력된 것을 확인할 수 있다.(도미(1): 10개, 빙어(0): 3개) 
- 하지만 여전히 약간의 `샘플링 편향`이 존재한다. 여기서는 빙어의 샘플이 부족하다. (class 비율이 일정하지 않은 경우 발생 -> Imblanced Data 문제)
- 무작위로 데이터를 섞는 경우, 이러한 샘플링 편향 문제가 발생할 수 있다. -> 올바르게 데이터 학습이 되지 않을 수 있다. 
- *`stratify`* 매개변수에 target 데이터를 전달하면 클래스 비율에 맞게 데이터 분할이 가능하다.


```python
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify=fish_target, random_state=42)
```


```python
print(test_target)
```

    [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.]


- 빙어(0)가 1개 증가하여 비율이 2.5:1로 개선되었다.

### 수상한 도미 한 마리 (이상 데이터 확인)
---
- K-nearest neighbor로 훈련


```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()
kn.fit(train_input, train_target)
kn.score(test_input, test_target)
```




    1.0



- 도미와 빙어 데이터셋에 대해 모델이 100%의 정확도 달성했다.


```python
# 도미 사이즈의 데이터셋을 예측한 결과 
print(kn.predict([[25, 150]]))
```

    [0.]


- 하지만, 도미 사이즈의 길이, 무게 값이 주어졌을 때 해당 데이터를 빙어(0) class로 분류하는 문제가 발생하였다. 
- 해당 문제의 원인은 `데이터의 스케일`을 조정하지 않았기 때문이다.
- 이를 확인하기 위해 데이터의 산점도 그래프를 그리면 다음과 같다.


```python
import matplotlib.pyplot as plt 

plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](output_26_0.png)
    


- 예측 데이터(삼각형)가 어떤 class와 더 가까이 있는지 거리를 확인하기 위해, kneighbors() 메소드를 이용하여 거리를 측정하여 표시하면 다음과 같다.


```python
distances, indexes = kn.kneighbors([[25, 150]])
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](output_28_0.png)
    


- 삼각형 샘플 데이터와 가장 가까운 5개의 샘플이 초록색 마름모로 표시되어 있다. 
- 가장 가까운 이웃에 도미 데이터는 1개, 나머지 4개는 빙어 데이터로 표시된다.
- 정확한 분석을 위해 distance 값을 출력해 보면 다음과 같다.


```python
print(distances)
```

    [[ 92.00086956 130.48375378 130.73859415 138.32150953 138.39320793]]


- 그래프 상에서 가장 가까운 도미 데이터와의 거리는 92이다. 그리고 나머지 빙어 데이터와의 거리는 130~138로 측정되었다.
- 그런데 이러한 값은 그래프 상에서 비율적으로 맞지 않다. 거리 92와 거리 130~138이 그래프 상에서 너무나 큰 차이가 존재하기 때문이다.
- `문제의 원인은 length와 weight feature의 스케일(크기 범주)에 차이`가 있기 때문이다. 

### 기준을 맞춰라
---
- 앞서 언급한 바와 같이 그래프상에서 92와 130~138은 그 비율에 문제점이 있다. 
- 이를 확인하기 위해 산점도 그래프를 다시 그려보면 다음과 같다.


```python
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlim((0, 1000))
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](output_33_0.png)
    


- 이러한 문제가 발생하는 이유는 x축의 범위(10 ~ 40)가 좁고, y축의 범위(0 ~ 1000)가 넓기 때문에 발생하는 문제이다.
- 따라서 y축으로 조금만 멀어져도 아주 큰 거리 값으로 계산되는 문제가 발생한다. 결과적으로 이러한 문제로 도미 샘플이 이웃으로 선택되지 못하게 된다.
- 따라서 feature 간 스케일을 비슷한 범주로 맞추어주는 작업이 데이터 전처리에서 굉장히 중요하다.
- 가장 널리 사용되는 데이터 스케일링 방법으로 `표준점수(standard score 또는 Z-score)`가 있다. 
  - 표준점수는 각 feature 값이 0에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. 
  - 표준편차를 통해 실제 특성값의 범주와 관계 없이 동일한 기준으로 값의 범위를 비교할 수 있게 된다. 

$$
z = \frac { x - μ  } { σ }
$$

- z: 표준점수 (각 데이터가 원점에서 몇 표준편차만큼 떨어져 있는지를 표현한 값) 
- x: 실제 값 (실제 데이터 값)
- μ: 평균
- σ: 표준편차 (분산의 제곱근으로 데이터가 분산된 정도를 의미) 

### np.mean(), np.std()를 이용한 평균 및 표준편차 계산
---


```python
mean = np.mean(train_input, axis=0) # axis=0을 열을 기준으로 계산한다는 의미
std = np.std(train_input, axis=0)
```


```python
# 계산된 평균과 표준편차 결과
print(mean, std)
```

    [ 27.29722222 454.09722222] [  9.98244253 323.29893931]



```python
# z-score 공식을 이용하여 scaled 값 계산
train_scaled = (train_input - mean) / std 
print(train_scaled)
```

    [[ 0.24070039  0.14198246]
     [-1.51237757 -1.36683783]
     [ 0.5712808   0.76060496]
     [-1.60253587 -1.37766373]
     [ 1.22242404  1.45655528]
     [ 0.17057727 -0.07453542]
     [ 0.87180845  0.80390854]
     [ 0.87180845  1.22457184]
     [ 0.37092904  0.06465464]
     [ 0.77163257  0.82246721]
     [ 0.97198434  1.68853872]
     [-1.61255346 -1.3742613 ]
     [ 0.72154463  0.51315596]
     [-1.53241275 -1.3742613 ]
     [ 0.17057727 -0.28177396]
     [ 0.5712808   0.76060496]
     [ 0.34087627  0.14198246]
     [ 1.12224816  1.54934866]
     [ 0.62136874  0.60594934]
     [-1.30200822 -1.34363949]
     [ 0.42101698  0.14198246]
     [-0.19005591 -0.65604058]
     [-1.75279969 -1.38384995]
     [ 0.47110492  0.45129371]
     [-1.68267658 -1.38137546]
     [ 0.62136874  0.48222484]
     [-1.67265899 -1.38292202]
     [ 0.77163257  0.76060496]
     [ 0.47110492  0.45129371]
     [ 0.77163257  0.83793278]
     [-1.43223687 -1.36683783]
     [ 0.27075315 -0.01267317]
     [ 0.47110492 -0.35291555]
     [-1.2318851  -1.34302087]
     [ 0.27075315 -0.19825992]
     [ 1.37268787  1.61121091]]


- numpy의 `broadcasting`에 의해 각 행에 대해 모든 계산이 자동으로 적용된다.

### 전처리 과정을 거친 데이터로 모델 훈련
---
- z-score를 적용한 데이터를 다시 모델에 학습하여 결과를 확인한다. 


```python
# 스케일 적용 전
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](output_42_0.png)
    



```python
# 스케일 적용 후
new = ([25, 150] - mean) / std
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](output_43_0.png)
    


- 데이터의 범주가 -1.5 ~ 1.5 사이값으로 스케일이 맞춰진 것을 확인할 수 있다.


```python
# 데이터 학습
kn.fit(train_scaled, train_target)
```




<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>




```python
test_scaled = (test_input - mean) / std
```

- 테스트 세트도 동일하게 스케일을 조정해주어야 한다.


```python
kn.score(test_scaled, test_target)
```




    1.0



- 모든 데이터에 대해 정확하게 예측한 결과를 확인할 수 있다.
- 해당 모델을 이용하여 다시 [25, 150] 샘플이 도미인지 빙어인지 예측해 본다.


```python
print(kn.predict([new]))
```

    [1.]


- 스케일 적용 전에는 빙어(0)로 분류되었던 샘플이 도미(1)로 정상적으로 분류되었다.
- 이를 통해 데이터의 스케일을 맞춰주는 것이 모델 학습에 어떤 영향을 미치는지 확인할 수 있다. 
