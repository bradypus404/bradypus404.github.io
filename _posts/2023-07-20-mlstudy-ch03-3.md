---
layout: post
title: "[ML Study]ch03-3 특성 공학과 규제"
subtitle: 
date: 2023-07-19 09:00:00 +0900
author: Jinha_k
categories: [Machine Learning]
tags: [Multiple Regression, Feature Engineering, scikit-learn, ridge, regularization]
published: false
---

## 다중 회귀(Multiple Regression)
**여러 개**의 특성을 사용한 선형 회귀이다. 하나의 특성을 사용하여 일직선만 사용하는 것이 아닌 두 개 이상의 특성을 사용하여 선을 그린다.

## 특성 공학(Feature Engineering)
특성들을 특성 그대로만 사용하지 않고 특성과 특성을 곱해서 또 다른 특성을 만들거나 특성 하나를 여러 방식대로 전처리하여 새로운 특성을 추출하는 작업을 특성 공학이라고 부른다.

## 데이터 준비
**판다스(pandas)**: 쉽고 직관적인 관계형 또는 분류된 데이터로 작업할 수 있도록 설계되었고 빠르고, 유연한 데이터 구조를 제공하는 Python 패키지이다.
**데이터프레임(Dataframe)**: 데이터를 행과 열로 구성된 2차원 테이블이다.
> CSV file -> pandas -> Dataframe
> pd.read_csv -> df.to_numpy
```python
import pandas as pd
df = pd.read_csv('http://bit.ly/perch_csv_data')
perch_full = df.to_numpy()
print(perch_full[:3], '\n\t...\n', perch_full[-2:]) 
```
```
[[ 8.4   2.11  1.41]
 [13.7   3.53  2.  ]
 [15.    3.82  2.43]] 
	...
 [[43.5  12.6   8.14]
 [44.   12.49  7.6 ]]
```
타겟 데이터 준비 및 훈련, 테스트 세트 분할을 진행한다.

```python
import numpy as np
from sklearn.model_selection import train_test_split

perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])

train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42)
```

## 사이킷런의 변환기
**사이킷런(scikit-learn)**: 대표적인 python 기계 학습 라이브러리이다. 현재도 scikit-learn을 활용하여 개발이 이우어지고 있어 정보 찾기에 수월하여 초심자가 기계 학습을 배우기 시작할 때 적합하다.
**변환기(transformer)**: 특성을 만들거나 전처리하기 위한 다양한 클래스를 제공한다.

### 변환기 예제
PolynomialFeatures 클래스
```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(include_bias=False)
poly.fit([[2, 3]])
print(poly.transform([[2, 3]]))
```
`[[2. 3. 4. 6. 9.]]`

우선 `fit([[2, 3]])`을 이용해서 훈련을 시켜준 후 변환기로 실행하면 다음과 같이 나옵니다. 꼭 훈련을 한 후 변환을 해야 한다. fit -> transfrom, 이 클래스는 기본적으로 제곱한 항을 추가하고 특성끼리 곱한 항을 추가한다. 여기서 데이터가 변형된 정확한 계산법은 아래와 같다.
> `include_bias=False`를 한 이유는 사이킷런 선형 모델은 자동으로 절편을 추가하므로 굳이 1이라는 특성을 만들 필요가 없기 때문이다. (?)

```python
poly.get_feature_names_out()
```
`array(['x0', 'x1', 'x0^2', 'x0 x1', 'x1^2'], dtype=object)`

### 실제 데이터 변환
train, test 데이터 세트를 훈련 후 변환하고 특성이 어떤 조합으로 만들어졌는지 확인한다.
```python
poly = PolynomialFeatures(include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
poly.get_feature_names_out()
```
`array(['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2','x2^2'], dtype=object)`

## 다중 회귀 모델 훈련
다중 회귀 모델을 훈련하는 것은 선형 회귀 모델을 훈련하는 것과 같다. 다만 여러 개의 특성을 이용하여 선형 회귀를 수행한다. 여기서는 LinearRegression 클래스를 사용한다.
```python
from sklearn.linear_model import LinearRegression
lr = LinearRegression() 
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))
```
```
0.9903183436982126  # train score
0.9714559911594125  # test score
```
**과대적합**, **과소적합** 문제는 발생하지 않고 점수가 높게 나온것으로 확인된다.

특성이 많으면 많을 수록 점수가 높게 나오는 것 같아서 degree(제곱)을 5로 설정하여 5제곱까지 특성을 만들어서 학습을 시켜보겠다.
```python
poly = PolynomialFeatures(degree=5, include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
print(train_poly.shape)
```
`(42, 55)`

특성은 총 55개로 기존에 9개였던 것보다 상당히 증가한 것을 볼 수 있다. 그리고 학습을 하여 점수를 살펴보겠다.
```python
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))
```
```
0.9999999999997232
-144.40564483377855
```
특성의 개수가 높으면 점수가 높게 나온다는 가설과는 다르게 훈련 세트에만 과하게 학습되어 **과대적합**이 발생하여 테스트 세트로는 정답을 맞추는 것이 불가능해 졌다. 그것을 해결하는 방법을 규제라고 하고 바로 밑에서 규제에 대해 살펴보겠다.

## 규제(regularization)
**규제**는 머신러닝 모델이 훈련세트에서 너무 과도하게 학습하기 못하도록 훼방을 놓는 것이라고 한다. 즉, 훈련 세트에 **과대적합**되지 않도록 만드는 일이다. 앞선 실습과 같이 특성을 과하게 늘려 훈련세트에 정확히 맞도록 하는 것을 **과대적합**을 발생하게 할 확률이 높다.<br>
우선 규제를 적용하기 위해 `StandardScaler` 클래스를 이용하여 정규화를 진행한다.
```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_poly)
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```
정규화한 데이터(표준점수)를 이용해 선형 회귀 모델에서 규제를 추가한 모델인 **릿지(ridge)**와 **라쏘(lasso)**를 이용해 정리한다.

### 릿지 회귀
간단하게 릿지 회귀는 계수를 제곱한 값을 규제로 적용한다. 항상 똑같은 방식인 모델 객체를 만들고 훈련을 한 후 점수를 평가한다.
```python
from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))
```
```
0.9896101671037343  # train score
0.9790693977615387  # test score
```
train 점수는 규제를 하기 전보다 낮아졌지만 test 점수는 규제를 한 후가 근소하게 증가한 것을 볼 수 있다.<br><br>
규제 모델을 사용할 때는 위와 같이 자동으로 훈련할 수도 있고 규제의 양을 임의로 조절할 수도 있다. 임의로 조절할때는 alpha값을 변경하여 조절한다. 이렇게 모델이 학습할 수 없고 사람이 알려줘야 하는 파라미터를 **하이퍼파라미터(hyperparameter)**라고 부른다.
> alpha ↑, 계수 값 ↓, 과소적합 ↑ || alpha ↓, 계수 값 ↑, 과대적합 ↑

적절한 alpha 값을 찾는 방법은 R<sup>2</sup>값의 그래프를 그려보는 것이다. 그래프를 비교하여 훈련 세트와 테스트 세트의 점수가 가장 가까운 지점이 최적의 alpha 값이 된다.
```python
import matplotlib.pyplot as plt 
train_score = []
test_score = []

alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
	ridge = Ridge(alpha=alpha)  # 릿지 모델 생성
	ridge.fit(train_scaled, train_target)   # 릿지 모델 훈련
	train_score.append(ridge.score(train_scaled, train_target)) # 훈련 점수, 테스트 점수 저장
	test_score.append(ridge.score(test_scaled, test_target))
```
위 코드는 alpha 값을 0.001에서 100까지 10배씩 늘려가며 훈련한 모델을 train, test 세트의 점수를 각각 저장한다.

```python
plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```
![ridge](/assets/images/post/2023-07-20-[3-3]/(3-3)-1_ridge_alpha.png)

그림과 같이 -1(alpha=0.1)일때 train, test 값의 차이가 제일 적어지는 것을 볼 수 있다.

```python
ridge = Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))
```
```
0.9903815817570367  # train score
0.9827976465386928  # test score
```
train, test 세트의 점수가 비슷하게 높고 과대적합과 과소적합이 균형을 맞추고 있다.

### 라쏘 회귀
간단하게 라쏘 회귀는 계수의 절대값을 기준으로 규제를 적용한다. 릿지 회귀와 똑같은 방식에서 객체만 바꿔주면 된다.

```python
from sklearn.linear_model import Lasso

lasso = Lasso() # Lasso 모델로 객체 생성
lasso.fit(train_scaled, train_target)
print('origin:', lasso.score(train_scaled, train_target))
print('origin:', lasso.score(test_scaled, test_target))

train_score = []
test_score = []
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
	lasso = Lasso(alpha=alpha, max_iter=10000)
	lasso.fit(train_scaled, train_target)
	train_score.append(lasso.score(train_scaled, train_target))
	test_score.append(lasso.score(test_scaled, test_target))
	
plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```
```
origin: 0.989789897208096   # train score
origin: 0.9800593698421883  # test score
```
ridge와 같이 기존의 점수들을 출력한 후 alpha에 대한 값을 그래프로 출력한다.

![lasso](/assets/images/post/2023-07-20-[3-3]/(3-3)-2_lasso_alpha.png)

파란색이 train 세트이고 주황색이 test 세트이다. 왼쪽은 train 세트에 과대적합되는 모습을 보여주고 오른쪽으로 갈수록 train, test 세트의 점수가 좁혀지고 있다가 더 가면 점수가 급격하게 떨어지는데 이 지점은 과소적합되는 부분이다. 그러므로 급격하게 떨어지기 전인 1이 최적의 alpha 값이다. 1은 10을 나타내므로 alpha를 10으로 지정하고 다시 훈련한다.

```python
lasso = Lasso(alpha=10)
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
```
```
0.9888067471131867  # train score
0.9824470598706695  # test score
```
기존의 값과 비교했을때 Lasso 모델을 사용했을때가 train, test score의 차이가 근소하고 전체적으로 점수가 높은 것을 볼 수 있다.
