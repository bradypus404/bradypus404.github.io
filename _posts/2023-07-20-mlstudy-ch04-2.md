---
layout: post
title: "[ML Study]ch04-2 확률적 경사 하강법"
subtitle: 혼자 공부하는 머신러닝+딥러닝 [4-2]
date: 2023-07-20 09:03:00 +0900
author: Hyeonsu
categories: [Machine Learning]
tags: [확률적 경사 하강법, 손실 함수, 에포크]
published: false
---

### 1. 점진적인 학습
---------------------------

훈련 데이터가 한 번에 준비되는 것이 아니라 조금씩 전달되어 문제가 생긴다.
<br>해결 방안으로 기존의 훈련 데이터에 새로운 데이터를 추가하여 모델을 매일 다시 훈련시키는 방법을 제안한다.
<br>해결 방안으로 기존의 훈련 데이터에 새로운 데이터를 추가하여 모델을 매일 다시 훈련시키는 방법을 제안한다.
하지만, 시간이 지날수록 데이터가 늘어나기 때문에 지속 가능한 방법이 아니다.

다른 방법으로 새로운 데이터를 추가할 때 이전 데이터를 버림으로써 훈련 데이터 크기를 일정하게 유지하는 것이 있다.
<br>데이터셋의 크기가 너무 커지지 않지만 다른 데이터에 없는 중요한 생선 데이터가 포함된 데이터를 버리면 문제가 생긴다.

따라서, 이전에 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 훈련하는 방법을 제안한다.

이를 **점진적 학습** 또는 온라인 학습이라 한다.
<br>대표적인 점진적 학습에는 **확률적 경사 하강법**이 있다.

#### 1-1. 확률적 경사 하강법
---------------------------

**확률적 경사 하강법**의 방식은 가장 가파른 경사를 따라 원하는 지점에 도달하는 것이 목표이다.
<br>가장 가파른 길을 찾아 내려오지만 조금씩 내려오는 것도 중요하다.

훈련 세트에서 랜덤하게 하나의 샘플을 고르는 것이 **확률적 경사 하강법**이다.
<br>훈련 세트에서 랜덤하게 하나의 샘플을 선택하여 가파른 경사를 조금 내려간다.
<br>이런 방법으로 전체 샘플을 모두 사용할 때까지 진행한다.

만약, 산을 다 내려오지 못했다면 처음부터 다시 시작해 훈련 세트에 모든 샘플을 다시 채워 넣는다.
<br>훈련 세트를 한 번 모두 사용하는 과정을 **에포크**라고 한다.
<br>일반적으로 수십, 수백 번 이상 에포크를 수행하는 것이다.

여러 개의 샘플을 사용해 경사 하강법을 수행하는 방식을 **미니배치 경사 하강법**이라 한다.
<br>한 번 경사로를 따라 이동하기 위해 전체 샘플을 사용하는 것을 **배치 결사 하강법**이라 한다.
<br>이는 전체 데이터를 사용하기 때문에 가장 안정적인 방법이 될 수 있다.
<br>그러나, 전체 데이터를 사용하면 컴퓨터 자원을 많이 사용하기에 한 번에 전체 데이터를 모두 읽을 수 없는 경우가 발생할 수 있다.

#### 1-2. 손실 함수
---------------------------

가장 빠른 길을 찾아 내려가려고 하는 산을 **손실 함수**라고 부른다.
<br>이는 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지 측정하는 기준이다.

손실 함수의 값이 작을수록 좋지만, 어떤 값이 최솟값인지는 알지 못한다.

분류에서의 손실은 정답을 못맞추는 것으로 
<br>도미는 양성 클래스(1), 빙어는 음성 클래스(0)이라 했을 때,

**[예측 : 정답(타깃)] -> [1 = 1], [0 != 1], [0 = 0], [1 != 0]**
<br>위의 정확도는 **0.5**이다.

이처럼 정확도를 측정할 때, 4개의 샘플만 있다면 가능한 정확도는 **0, 0.25, 0.5, 0.75, 1**로 다섯 가지뿐이다.

연속적인 손실 함수로 **로지스틱 회귀**가 있다.

#### 1-3. 로지스틱 손실 함수
---------------------------

샘플 4개의 예측 확률을 **0.9, 0.3, 0.2, 0.8**이라 가정한다면,

`(1) 0.9 x 1 -> -0.9`와 같이 양성 클래스인 1과 곱한 다음 음수로 바꿔준다.
<br>예측이 1에 가까울수록 예측과 타깃의 곱의 음수는 점점 작아져 좋은 모델이 된다.

`(2) 0.3 x 1 -> -0.3` -0.9보다 높은 손실을 보여준다.

0.2 샘플의 타깃은 음성 클래스로 0이다.
<br>그러므로 양성 클래스로 바꾸어 계산하면 (1 - 0.2 = 0.8),
<br>`(3) 0.8 x 1 -> -0.8`로 꽤 낮은 손실이다.

0.8도 음성 클래스이므로 0.2로 계산한다면,
<br>`(4) 0.2 x 1 -> -0.2`로 높은 손실을 보여준다.

결국, **(1), (3)**은 낮은 손실, **(2), (4)**는 높은 손실이다.

예측 확률에 로그 함수를 적용하면 더 좋아진다.
<br>로그 함수는 0~1 사이인 예측 확률이 범위에서 음수가 되므로 최종 손실 값은 양수가 된다.

![log](/assets/images/post/2023-07-20-[4-2]/ch04-2(1).png)

양성 클래스 (타깃 = 1)일 때, 손실은 -log(예측 확률)로 계산한다.
<br>0에 가까워질수록 손실은 아주 큰 양수가 되는 것이다.

음성 클래스 (타깃 = 0)일 때, 손실은 -log(1 - 예측 확률)로 계산한다.
<br>1에 가까워질수록 손실은 아주 큰 양수가 되는 것이다.

이러한 손실 함수를 **로지스틱 손실 함수** 또는 **이진 크로스엔트로피 손실 함수**라고 부른다.
<br>다중 분류도 매우 비슷한 손실 함수를 사용하는데 이를 **크로스엔트로피 손실 함수**라고 부른다.

이제 확률적 경사 하강법을 사용한 분류 모델을 만들어 보자.




### 2. SGDClassifier
---------------------------

fish_csv_data 파일에서 판다스 데이터프레임을 만들어 보자.
```python
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')
```

Species열(타깃 데이터)을 제외한 나머지 5개는 입력 데이터로 사용한다.
```python
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')
```

train_test_split() 함수 사용해서 훈련 세트와 데이터 세트로 나눈다.
```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
```
두 특성을 표준화 전처리한다.
*훈련 세트에서 학습한 통계 값으로 데스트 세트도 변환해야 한다.
```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


확률적 경사 하강법을 제공하는 대표적인 분류용 클래스는 **SGDClassifier**이다.
```python
from sklearn.linear_model import SGDClassifier
```
loss='log_loss'로 지정해 로지스틱 손실 함수 지정한다.
max_iter은 수행할 에포크 횟수를 지정한다.
```python
sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```
![warning](/assets/images/post/2023-07-20-[4-2]/ch04-2(2).png)
<br> 정확도가 두 개 모두 낮다.
<br> 지정한 반복 횟수 10번이 부족한 것으로 보인다.

**SGDClassifier** 객체를 다시 만들지 않고 훈련한 모델 sc를 추가로 훈련해 보자.
<br>이어서 훈련할 때는 partial_fit()메서드 사용한다.
<br>fit()과 같지만 호출할 때마다 1 에포크씩 이어서 훈련이 가능하다.

```python
sc.partial_fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```
![0.85](/assets/images/post/2023-07-20-[4-2]/ch04-2(4).png)
에포크를 한 번 더 실행하니 정확도가 향상된 것을 볼 수 있다.



### 3. 에포크와 과대/과소적합
---------------------------

![3](/assets/images/post/2023-07-20-[4-2]/ch04-2(3).png)
훈련 세트 점수는 에포크가 진행될수록 꾸준히 증가,
<br>테스트 세트 점수는 어느 순간 감소하기 시작한다.
<br>이 지점이 과대적합되기 시작하는 곳이다.

과대적합이 시작하기 전에 훈련을 멈추는 것을 **조기 종료**라고 한다.

훈련 세트에 있는 전체 클래스의 레이블을 partial_fit() 메서드에 전달해주어,
<br>partial_fit() 메서드만 사용한다.

np.unique() 함수로 train_target의 7개 생선 목록을 만든다.
<br>각 점수를 기록하기 위해 2개의 리스트도 준비한다.
```python
import numpy as np
sc = SGDClassifier(loss='log_loss', random_state=42)
train_score = []
test_score = []
classes = np.unique(train_target)
```
300번의 에포크 동안 훈련을 반복해보자.
```python
for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))
```

```python
import matplotlib.pyplot as plt
plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```
![show](/assets/images/post/2023-07-20-[4-2]/ch04-2(5).png)

100번 째 에포크 이후에는 훈련 세트와 테스트 세트의 점수가 조금씩 벌어지고 있다.
<br>또한 에포크 초기에는 과소적합되어 두 점수가 모두 낮다.
<br>따라서, 100번 째 에포크가 적절한 반복 횟수로 보인다.

```python
sc = SGDClassifier(loss='log_loss', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```
![0.925](/assets/images/post/2023-07-20-[4-2]/ch04-2(6).png)
