---
layout: post
title: "[ML Study]ch05-1 결정 트리"
subtitle: 혼자 공부하는 머신러닝+딥러닝 [5-1]
date: 2023-07-27 09:00:00 +0900
author: Hyemin
categories: [Machine Learning]
tags: [결정 트리, 불순도, 정보 이득, 가지치기, 특성 중요도]
---
<body>
    <img
    src="/assets/images/post/book_banner.jpg"
    align="right"
    width="20%"
    height="27.2%"
    />
    <br><br>
    <p>본 포스팅 한빛미디어의 <a href="https://product.kyobobook.co.kr/detail/S000001810330"><혼자공부하는 머신러닝+딥러닝(박해선 저)></a>를 요약 정리했습니다.</p>
</body>
<br>

## 결정 트리
------------------
### 결정 트리

**결정 트리(Decision Tree)** 
    1. 결정 트리 모델은 분류와 회귀가 모두 가능한 **지도 학습 모델**이다.

    2. 스무 고개 하듯이 예/아니오 와 같은 질문을 이어가며 학습을 진행한다.

    3. 데이터를 잘 나눌 수 있는 질문을 찾는다면 계속 질문을 추가해서 분류 정확도를 높일 수 있다.
    
![결정트리예시](/assets/images/post/2023-07-26[5-1]/1.결정트리예시.png)

- 위의 그림과 같이 예/아니오 와 같은 질문을 이어가며 학습을 진행한다.
- 사이킷런에서는 *DecisionTreeClassifier* 클래스를 통해 결정 트리 알고리즘을 제공한다.
- 아래의 그림은 클래스를 이용하여 결정 트리를 그린 모습이다.

![결정트리](/assets/images/post/2023-07-26[5-1]/2.결정트리.png)

- 각 네모칸을 트리의 노드(node)라고 하며, 맨 위의 노드는 루트 노드(root node), 마지막 노드는 리프 노드(leaf node)라고 부른다. 선은 에지(edge)라고 부르며 질문이 답과 다음 질문은 연결한다.
- 노드는 결정 트리를 구성하는 핵심 요소이며, 훈련 데이터의 특성에 대한 테스트를 표현한다.
- 일반적으로 하나의 노드는 2개의 가지를 가진다.

![결정트리_2](/assets/images/post/2023-07-26[5-1]/3.결정트리_2.png)

- 위 트리는 전체 트리에서 깊이를 제한하여 출력한 것이다.
- 위 트리의 노드가 가지고 있는 정보는 아래의 그림과 같다.

![노드 정보](/assets/images/post/2023-07-26[5-1]/4.노드정보.png)

- 노드를 하나씩 살펴보면, 루트노드는 꽃잎의 길이가 2.45 이하인지 질문을 한다. 만약 샘플의 꽃잎 길이가 2.45와 같거나 작으면 오른쪽으로 이동해 또 다른 질문을 하며 이어나거나, 끝(리프 노드)에 도달한다.
- 여기서 색의 진함은 클래스의 비율의 따라 달라진다. 비율이 높아지면 진한 색으로 표시되고, 비율이 낮으면 연한 색으로 표시된다.
- 결정 트리에서 예측하는 방법은 리프 노드에서 가장 많은 클래스가 예측 클래스가 된다. 

------------------
### 불순도

**불순도**란 다양한 범주들의 개체들이 얼마나 포함되어 있는지를 의미한다.
노드에 gini 라는 것이 있는데, gini는 **지니 불순도(gini impurity)**를 의미한다. DecisionTreeClassifier 클래스의 criterion 매개변수의 기본값이 'gini'이다. 

지니 불순도 계산은 다음과 같다.

**지니 불순도 = 1 - (음성 클래스 비율² + 양성 클래스 비율)²**

지니 불순도를 계산하여 0에 가까운 값이 되면 순수 노드라고 부르고, 1에 가까운 값이 되면 순수하지 않은 노드라고 부른다.

결정 트리 모델은 부모 노드(parent node)와 자식 노드(child node)의 불순도 차이가 가능한 크도록 트리플 성장 시킨다.

부모 노드와 자식 노드의 불순도 차이를 계산하는 방식은 먼저 자식 노드와 불순도를 샘플 개수에 비례하여 모두 더한 후, 부모 노드의 불순도에서 뺴면 된다. 

아래와 같은 식으로 계산한다.

**부모의 불순도 - (왼쪽 노드 샘플 수 / 부모의 샘플 수) * 왼쪽 노드 불순도 - (오른쪽 노드 샘플 수 / 부모의 샘플 수) * 오른쪽 노드 불순도**

이런 부모와 자식 노드 사이의 불순도 차이를 **정보이득(information gain)**이라고 부른다.

사이킷런에는 또 다른 불순도 기준이 있다.

DecisionTreeClassifier 클래스에서 criterion='entropy'를 지정하여 엔트로피 불순도를 사용할 수 있다.

엔트로피 불순도도 노드의 클래스 비율을 사용하지만 지니 불순도처럼 제곱이 아니라 밑이 2인 로그를 사용하여 곱한다.

예를 들어 루트 노드의 엔트로피 불순도는 아래와 같은 계산 방법으로 계산할 수 있다.

**-음성 클래스 비율 *  log2(음성 클래스 비율) - 양성 클래스 비율 * log2(양성 클래스 비율)**

------------------
### 가지 치기

결정 트리 모델을 생성할 때 최대한 정확한 훈련 샘플의 분류를 위해 노드 분할과정이 지속적으로 반복된다.

하지만 많은 반복은 가지 수를 지나치게 많이 만들게 된다.

이때 발생하는 문제는 과적합이며, 가지를 잘라 과적합이 되지 않도록 만들어야한다.

이때는 사용자가 임의로 불순도 정도를 설정하여 해당 지니/엔트로피에 도달하면 트리를 그리기를 멈추게 할 수 있다.



